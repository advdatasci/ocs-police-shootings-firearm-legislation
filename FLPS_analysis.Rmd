---
title: Firearm Legislation and Fatal Police Shootings in the US
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Alexandra Stephens

# Motivation
Here we will recreate a study completed by the American Journal of Public Health "to examine whether stricter firearm legislation is associated with rates of fatal police shootings." (https://ajph.aphapublications.org/doi/suppl/10.2105/AJPH.2017.303770)
The bulk of the case study will be collecting, cleaning, manipulating and merging data in order to control for potential covariates. Then we will perform a poisson regression as well as statistical significance tests.

# What is the data?
This study uses data from several different sources.

1. The Brady Campain State Scorecard from 2015. This provides numerical scores for each state across different types of firearm legislation. Obtained at http://crimadvisor.com/data/Brady-State-Scorecard-2015.pdf.
2. The Counted: People killed by police in the US. Because "the US government has no comprehensive record of the number of people killed by law enforcement," The Counted project started and is known as a more accurate source for this data. https://www.theguardian.com/us-news/ng-interactive/2015/jun/01/about-the-counted.
3. Population characteristics by state obtained from the US Census https://www.census.gov/content/census/en/data/tables/2017/demo/popest/state-detail.html.
4. Violent crime data in 2015 by state.Obtained from the FBI's Uniform crime report https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/tables/table-5.
5. Unemployment rate data: https://www.bls.gov/lau/lastrk15.htm
6. To calculate population density in 2015, we must import land area data from the US census (2010 land area data). https://www.census.gov/support/USACdataDownloads.html#LND
7. Need education data

# Data import
We import the Census population characteristics data, Brady Campaign Scorecard (2015), The Counted data for the years of 2015 and 2016, the FBI's crime report for 2015, and land area by state from the Census. These files were downloaded from their respective websites listed in the above section "What is the data?"
```{r}
library(readxl)
census <- read.csv("sc-est2017-alldata6.csv",nrows = 236900,stringsAsFactors = FALSE)
head(census[, 1:5])
brady <- read_excel("Brady-State-Scorecard-2015.xlsx", sheet = 1)
head(brady[, 1:5])
counted15 <- read.csv("the-counted-2015.csv",stringsAsFactors = FALSE)
counted16 <- read.csv("the-counted-2016.csv",stringsAsFactors = FALSE)
head(counted15)
crime <- read_excel("table_5_crime_in_the_united_states_by_state_2015.xls", sheet = 1,skip = 3)
head(crime[, 1:5])
land <- read_excel("LND01.xls", sheet = 1)
head(land[, 1:5])
```


# Data wrangling
## 1. Census Data
### Percent Male

The following is taken from the document sc-est2017-alldata6.pdf:

  * The key for SEX is as follows:
    + 0 = Total
    + 1 = Male
    + 2 = Female

  * The key for ORIGIN is as follows: 
    + 0 = Total
    + 1 = Not Hispanic
    + 2 = Hispanic

  * The key for RACE is as follows:
    + 1 = White Alone
    + 2 = Black or African American Alone
    + 3 = American Indian and Alaska Native Alone
    + 4 = Asian Alone
    + 5 = Native Hawaiian and Other Pacific Islander Alone 6 = Two or more races

So we need to group the data in several different ways and calculate the necessary statistics.
Let's start with "percentage of the study population that was male." For each state, we need to add every row of data that is "male" (SEX = 1) in the column POPESTIMATE2015 since we are looking at the year 2015.
We set the ORIGIN equal to 0 so that we do not add duplicate data, since 0 is the key for both Hispanic and non Hispanic residents.
We group by each state since we want the percentage of male resendents in each state.
We set the column "percentMale" to be the total male population divided by the total population in each state and then multiplied by 100 to get a percentage.
```{r}
library(dplyr)
maleStats <- census %>%
  filter(ORIGIN == 0) %>%
  group_by(NAME) %>%
  summarize(percentMale = sum(POPESTIMATE2015[SEX==1])/sum(POPESTIMATE2015[SEX==0])*100)
head(maleStats)
```
### Median Age
First we need to get the total counts for each age (per state), since that's divided up into all the other categories.
Set origin and sex to "Total" so we don't have repeats in the data.
Group by state and age since these are the two factors we need to keep seperated.
The column sumAges is the total residents in each specified state at each age year.
```{r}
library(dplyr)
library(tidyr)
ageStats <- census %>%
  filter(ORIGIN == 0,SEX == 0) %>%
  group_by(NAME,AGE) %>%
  summarize(sumAges = sum(POPESTIMATE2015))
head(ageStats)
```
We begin to see that finding the median is tricky, since we don't have a nice list of ages (i.e., [22,55,4,27,...,35]) for each state, in which case we could simply use a built in R "median" function. Instead we have the number of people that are ages 0-85+ in each state. For example, the first row of data above says that in Alabama (in 2015), there are 59080 residents that are 0 years old.

First let us transform the matrix to have each state as a column name with the sumAges data underneath. I.e., right now we have a 51*86 by 3 dataframe, and instaed we want a 51 by 86 dataframe. I removed the age column since we can use the index instead (index minus 1, since R dataframes are indexed from 1 and the ages start at 0).
```{r}
library(data.table)
ageStats <- dcast(setDT(ageStats), AGE ~ NAME, value.var="sumAges")
ageStats$AGE <- NULL
head(ageStats[,0:5])
```
Now that we've made the data easier to work with, we need to find a way to get the median. One method is to take the cumulative sum of each column and then divide all the rows by the last row in each repective column, calculating a percentile/quantile for each age. Then, we need to find the age in each state where the population exceeds the 50th percentile (the median!) for the first time.
```{r}
ageStats <- as.data.frame(apply(ageStats, 2, cumsum))
ageStats <- as.data.frame(apply(ageStats,2,function(x) x/x[86]))
head(ageStats[, 1:6])
median <- as.data.frame(apply(ageStats,2,function(x) which.max(x >= 0.5)-1))
head(median)
```
### Percent White, Black, and Hispanic
This is a simple calculation similar to caculating the percentage of male residents above.
```{r}
# library(dplyr)
race_origin_stats <- census %>%
  filter(ORIGIN == 0, SEX == 0) %>%
  group_by(NAME) %>%
  summarize(percentWhite = sum(POPESTIMATE2015[RACE==1])/sum(POPESTIMATE2015)*100,
            percentBlack = sum(POPESTIMATE2015[RACE==2])/sum(POPESTIMATE2015)*100) 

race_origin_stats$percentHispanic <- census %>%
  filter(SEX == 0) %>% 
  group_by(NAME) %>%
  summarize(x = sum(POPESTIMATE2015[ORIGIN==2])/sum(POPESTIMATE2015[ORIGIN==0])*100) %>%
  pull(x)

head(race_origin_stats)
```
## 2. Violent Crime
First print the names of the columns to see that there are some extraneous characters not visible in the dataframe. Use the column index to select columns instead of the complicated names.
Also, print a section violent crime to observe the X__1 group we are looking for -- "Rate per 100,000 inhabitants" (per the study.)
```{r}
#library(dplyr)
colnames(crime)
violentcrime<- crime[ , c(1,3,5)]
violentcrime[11, ]
```
We need get all rows where X__1 = Rate per 100,000 inhabitants. However, as we can see above, the value for "State" in these rows is NA. We need to fill that value with the state name that is listed in a previous row.
Then we can select the rows where X__1 = Rate per 100,000 inhabitants.
After that, we no longer need the column X__1, so we can remove it.
```{r}
library(zoo)
violentcrime$State <- na.locf(violentcrime$State)
violentcrime[11, ]
violentcrime<- subset(violentcrime, X__1 == "Rate per 100,000 inhabitants")
violentcrime$X__1 <- NULL
names(violentcrime) <- c("State", "violent_crime")
head(violentcrime)

```
Looking at our whole dataframe we can see some of the state names have numbers in them (an example is printed below). This will make it hard to later merge this data together, so we should clean this up.
```{r}
violentcrime$State[20]
violentcrime$State<-gsub('[0-9]+', '', violentcrime$State)
violentcrime$State[20]
```

## 3. Brady Scores
The study by AJPH groups the scores by the 7 categories. It is also important to note that the study removed all weightings of the different laws in favor of a "1 law 1 point" system, since the weightings were "somewhat arbitrary."

For the purpose of practice and simplification (for now), let's just keep the first line of "total state points" from the Brady Scorecard.

We need to transform the data frame so that we have a column of state names, and the total scores in another column.
```{r}
head(brady)
colnames(brady)[1] <- "Law"
brady <- brady %>%
  filter(Law == "TOTAL STATE POINTS")
brady <- brady[,(ncol(brady) - 49):ncol(brady)]
brady_totals <- as_data_frame(colnames(brady))
brady_totals$brady_totals <- t(brady)
colnames(brady_totals)[1] <- "state"
head(brady_totals)
```
In addition, this data frame's list of states is in two-letter state abbreviations. Since this is not cohesive with the previous datasets, we should change it to make merging dataframes easier later on. We use the openintro library to do the simple conversion (found by Googling!)
```{r}
library(openintro)
brady_totals$state <-toupper(abbr2state(brady_totals$state))
head(brady_totals)
```

## 4. The Counted Fatal Shootings
First we simply need to concatenate the two years of data.
```{r}
counted1516 <- rbind(counted15, counted16)
head(counted1516)
```
Again, this data frame's list of states is in two-letter state abbreviations so let's change that.
```{r}
#library(openintro)
counted1516$state <-toupper(abbr2state(counted1516$state))
```
In the study, researchers "calculated descriptive statistics for the proportion of victims that were male, armed, and non-White." Thus we repeat here easily with dplyr. "Tally" is used to count the number of observations in each group.
```{r}
fatal_gunshot <- counted1516 %>%
  filter(classification == "Gunshot",raceethnicity != "white", armed != "No", gender == "Male") %>%
  group_by(state) %>%
  tally()
head(fatal_gunshot)
```
## 5. Unemployment Data
This data is available online from the BLS, but there is no easy download of the table. It is also difficult to simply copy and paste; it doesn't hold it's table format. Thus, let's set up a web scraper to get the data.

To do this, we must use the *rvest* package. To view the HTML of a webpage, right-click and select "View page source."

Then we need to get the values from each column of the data table. *html_nodes* acts as a CSS selector. The sub0 class returns the state names, and the datavalue class corresponds to the values of both columns, Unemployment Rank and Rate. Since there is no differentiation of the two columns, and our "datavalues" alternate, we select subsets of this column to be the two seperate columns "rank" and "rate."
```{r}
library(rvest)
url <- read_html("https://www.bls.gov/lau/lastrk15.htm")
datavalues <- url%>%
  html_nodes('.datavalue') %>%
  html_text() %>%
  as.numeric()

state <- url%>%
  html_nodes('.sub0') %>%
  html_text()

unemployment<-as.data.frame(state[c(2:52)]) 
rate_select <- seq(3,103,by=2)
rank_select <- seq(4,104,by=2)
unemployment$rate <- datavalues[rate_select]
unemployment$rank <- datavalues[rank_select]
colnames(unemployment)[1] <- "state"
head(unemployment)
```
## 6. Population Density 2015
To manually find population density, we can calculate total 2015 population from the Census data and import land area to calculate population density. This is caculated because good data for state population in 2015 was not readily available.

I select LND110210D because I looked at the data table at https://www.census.gov/geo/reference/state-area.html and compared values to find the correct column. This one corresponds to land area in square miles.
```{r}
library(dplyr)
totalPop <- census %>%
  filter(ORIGIN == 0, SEX == 0 ) %>%
  group_by(NAME) %>%
  summarize(total = sum(POPESTIMATE2015))

landSqMi <- land %>%
  select(Areaname,LND110210D)

head(landSqMi)
```
Since *landSqMi* gives us area for each town, we need to do an inner merge on the state names so that we just get the area for each state.

I like to merge on columns with the same name so I rename the columns of *landSqMi* to match totalPop.

Then, convert the state names to be all uppercase, since this has been the format of the majority of these datasets, and we need the row values to be exactly equivalent to merge successfully.
```{r}
names(landSqMi) <- c("NAME", "LandAreaSqMiles")

landSqMi$NAME <- toupper(landSqMi$NAME)
totalPop$NAME <- toupper(totalPop$NAME)

popdensity <- merge(totalPop,landSqMi,by="NAME")
popdensity$density <- popdensity$total/popdensity$LandAreaSqMiles
head(popdensity)
```


# Exploratory data analysis


# Data analysis 

# Summary of results


